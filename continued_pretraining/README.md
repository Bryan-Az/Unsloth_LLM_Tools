# Continued Pretraining with Unsloth

In this section, I showcase how to utilize the LoRa adapter provided by Unsloth to implement 'Continued Pretraining' or CPT to reformat its' understanding to a new language. CPT differs from Finetuning/Continued Finetuning as it will generally attempt to guide the model to a new domain of knowledge, as opposed to a specific task as in finetuning. Generally, this might be useful when you would like the model to have the most up-to-date knowledge of some domain or field. As it is similar to training, it involves re-training a greater number of parameters than finetuning/continued-finetuning. Pairing continued-pretraining with continued-finetuning may result in competing parameters and could result in less performance if not done correctly.
