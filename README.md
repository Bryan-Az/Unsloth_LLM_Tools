# Unsloth_LLM_Tools

Unsloth is a free to use library that can be used to accelerate Large Language Model training, fine-tuning, and reward modelling. In this repo, I exemplify unsloth in a variety of use-cases.

## Continued Pre-training

[Video Presentation Link](https://youtu.be/Cwyw40mj_tQ)

## Fine-tuning

[Video Presentation Link: Base Finetuning](https://youtu.be/T62Ni3zltBM)

[Video Presentation Link: Continued Finetuning](https://youtu.be/Cwyw40mj_tQ)

## Reward Modelling

[Video Presentation Link](https://youtu.be/MO9nBYqqep8)

## Supplemented Chat

[Video Presentation Link](to-do)

# References

1. [Unsloth Documentation: Finetuning](https://colab.research.google.com/drive/1OBk5aACGK6GusGHaCWZSGu1C_eat12sE#scrollTo=pCqnaKmlO1U9)
2. [Unsloth Documentation: Continued Finetuning](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint)
3. [Unsloth Documentation: Continued Pretraining](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing#scrollTo=2eSvM9zX_2d3)
4. [Unsloth Documentation: Reward Modelling](https://docs.unsloth.ai/basics/reward-modelling-dpo-orpo-and-kto)

